{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:39:36.335965Z",
     "iopub.status.busy": "2024-04-09T04:39:36.335965Z",
     "iopub.status.idle": "2024-04-09T04:39:48.162216Z",
     "shell.execute_reply": "2024-04-09T04:39:48.162216Z",
     "shell.execute_reply.started": "2024-04-09T04:39:36.335965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d986e876cb7487e97a2d729f0c20a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 第八章作业\n",
    "# 作业2： 使用 AWQ 量化 Facebook OPT-6.7B 模型\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = 'D:/MTIDE/.cache/huggingface'\n",
    "os.environ['HF_HUB_CACHE'] = 'D:/MTIDE/.cache/huggingface/hub'\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "# 使用预先从 HuggingFace 下载的模型文件\n",
    "# model_name_or_path = \"facebook\\opt-6.7b\"\n",
    "model_name_or_path = \"D:/MTIDE/code/AI/models/facebook/opt-6.7b\"\n",
    "quant_model_dir = 'models/opt-6.7b-awq'\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}\n",
    "\n",
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:40:26.971951Z",
     "iopub.status.busy": "2024-04-09T04:40:26.970914Z",
     "iopub.status.idle": "2024-04-09T04:52:09.320365Z",
     "shell.execute_reply": "2024-04-09T04:52:09.320365Z",
     "shell.execute_reply.started": "2024-04-09T04:40:26.971951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mit-han-lab/pile-val-backup couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at D:\\MTIDE\\.cache\\huggingface\\datasets\\mit-han-lab___pile-val-backup\\default\\0.0.0\\2f5e46ae6a69cf0dce4b12f78241c408936ca0e4 (last modified on Tue Apr  9 09:46:00 2024).\n",
      "AWQ: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [11:19<00:00, 21.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:52:38.180087Z",
     "iopub.status.busy": "2024-04-09T04:52:38.180087Z",
     "iopub.status.idle": "2024-04-09T04:52:38.189102Z",
     "shell.execute_reply": "2024-04-09T04:52:38.189102Z",
     "shell.execute_reply.started": "2024-04-09T04:52:38.180087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transformers 兼容性配置abs\n",
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:52:42.344060Z",
     "iopub.status.busy": "2024-04-09T04:52:42.343563Z",
     "iopub.status.idle": "2024-04-09T04:52:44.653298Z",
     "shell.execute_reply": "2024-04-09T04:52:44.653298Z",
     "shell.execute_reply.started": "2024-04-09T04:52:42.343563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/opt-6.7b-awq\\\\tokenizer_config.json',\n",
       " 'models/opt-6.7b-awq\\\\special_tokens_map.json',\n",
       " 'models/opt-6.7b-awq\\\\vocab.json',\n",
       " 'models/opt-6.7b-awq\\\\merges.txt',\n",
       " 'models/opt-6.7b-awq\\\\added_tokens.json',\n",
       " 'models/opt-6.7b-awq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:53:03.948656Z",
     "iopub.status.busy": "2024-04-09T04:53:03.948656Z",
     "iopub.status.idle": "2024-04-09T04:53:03.965702Z",
     "shell.execute_reply": "2024-04-09T04:53:03.965702Z",
     "shell.execute_reply.started": "2024-04-09T04:53:03.948656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptAWQForCausalLM(\n",
       "  (model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
       "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (v_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (q_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (out_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): WQLinear_GEMM(in_features=4096, out_features=16384, bias=True, w_bit=4, group_size=128)\n",
       "            (fc2): WQLinear_GEMM(in_features=16384, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:53:12.663263Z",
     "iopub.status.busy": "2024-04-09T04:53:12.662264Z",
     "iopub.status.idle": "2024-04-09T04:53:14.585521Z",
     "shell.execute_reply": "2024-04-09T04:53:14.585521Z",
     "shell.execute_reply.started": "2024-04-09T04:53:12.663263Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用 GPU 加载量化模型\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:53:19.768248Z",
     "iopub.status.busy": "2024-04-09T04:53:19.768248Z",
     "iopub.status.idle": "2024-04-09T04:53:19.771141Z",
     "shell.execute_reply": "2024-04-09T04:53:19.771141Z",
     "shell.execute_reply.started": "2024-04-09T04:53:19.768248Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:53:23.477475Z",
     "iopub.status.busy": "2024-04-09T04:53:23.477475Z",
     "iopub.status.idle": "2024-04-09T04:53:24.992437Z",
     "shell.execute_reply": "2024-04-09T04:53:24.992437Z",
     "shell.execute_reply.started": "2024-04-09T04:53:23.477475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to hear you're enjoying it. :D\n",
      "Thank you and thank you again for the card and candy ;)\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T04:53:28.246049Z",
     "iopub.status.busy": "2024-04-09T04:53:28.245050Z",
     "iopub.status.idle": "2024-04-09T04:53:29.872173Z",
     "shell.execute_reply": "2024-04-09T04:53:29.872173Z",
     "shell.execute_reply.started": "2024-04-09T04:53:28.246049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a hairdresser?\n",
      "This is what I saw as well. It seems strange but I don't think she deserves this.\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
