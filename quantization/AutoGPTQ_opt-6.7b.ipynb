{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623063d-2d4e-466c-ab69-4dda9e920710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第八章作业\n",
    "# 作业1： 使用 GPTQ 量化 OPT-6.7B 模型\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = 'D:\\MTIDE\\.cache\\huggingface'\n",
    "os.environ['HF_HUB_CACHE'] = 'D:\\MTIDE\\.cache\\huggingface\\hub'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "# 使用预先从 HuggingFace 下载的模型文件\n",
    "# model_name_or_path = \"facebook\\opt-6.7b\"\n",
    "model_name_or_path = \"D:\\MTIDE\\code\\AI\\models\\facebook\\opt-6.7b\"\n",
    "model_dir = 'models/opt-6.7b-gptq'\n",
    "\n",
    "# 使用 GPTQ 算法支持的默认数据集来量化\n",
    "quantization_config = GPTQConfig(\n",
    "     bits=4, # 量化精度\n",
    "     group_size=128,\n",
    "     dataset=\"wikitext2\",\n",
    "     desc_act=False,\n",
    ")\n",
    "\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7e5ce-2c02-4e57-a4d4-206ae5a3d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a9274-0c8e-4d86-9049-8995d5140bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型权重\n",
    "quant_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b28d3b-dc87-47f9-a1e1-a9c3ca183ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 GPU 加载模型并生成文本\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5d963-bbbc-49bd-b03b-8f304b49fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义数据集量化模型\n",
    "\n",
    "from transformers import AutoModelForCausalLM, GPTQConfig, AutoTokenizer\n",
    "\n",
    "custom_dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n",
    "\n",
    "custom_quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    "    dataset=custom_dataset\n",
    ")\n",
    "\n",
    "custom_quant_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                          quantization_config=custom_quantization_config,\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bd2b7-ec8e-418a-bb7b-3dc623585e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = custom_quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
