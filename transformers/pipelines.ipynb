{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bad320-957b-48d2-a243-154f69fc4074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.6657696962356567}]\n",
      "[{'label': 'neutral', 'score': 0.6030055284500122}]\n",
      "[{'label': 'positive', 'score': 0.9461327791213989}]\n",
      "[{'label': 'positive', 'score': 0.7639099359512329}]\n",
      "[{'label': 'negative', 'score': 0.7824517488479614}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 使用本地下载的模型文件\n",
    "BASE_FOLDER = '/Users/mtide/Documents/code/AI/models/'\n",
    "\n",
    "# 1. 情感分析\n",
    "MDL_NAME = 'lxyuan/distilbert-base-multilingual-cased-sentiments-student'\n",
    "pipe = pipeline('sentiment-analysis',\n",
    "    model=BASE_FOLDER + MDL_NAME\n",
    ")\n",
    "\n",
    "r = pipe(\"今儿上海可真冷啊\")\n",
    "print(r)\n",
    "\n",
    "r = pipe(\"我觉得这家店蒜泥白肉的味道一般\")\n",
    "print(r)\n",
    "\n",
    "r = pipe(\"你学东西真的好快，理论课一讲就明白了\")\n",
    "print(r)\n",
    "\n",
    "r = pipe(\"You learn things really quickly. You understand the theory class as soon as it is taught.\")\n",
    "print(r)\n",
    "\n",
    "r = pipe(\"Today Shanghai is really cold.\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc54a6cf-9d72-49f1-92cc-54618e53c6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.7824517488479614},\n",
       " {'label': 'negative', 'score': 0.37749969959259033},\n",
       " {'label': 'positive', 'score': 0.7639099359512329},\n",
       " {'label': 'positive', 'score': 0.9461327791213989}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\",\n",
    "    \"你学东西真的好快，理论课一讲就明白了\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480872ce-2836-42e7-abe5-b0d7f35e77a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /Users/mtide/Documents/code/AI/models/dslim/bert-base-NER were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at /Users/mtide/Documents/code/AI/models/dslim/bert-base-NER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': 0.8935, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.915, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.9777, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'B-MISC', 'score': 0.9996, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'B-LOC', 'score': 0.9995, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9994, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-LOC', 'score': 0.9996, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "# 2. 命名实体识别（NER）\n",
    "\n",
    "# MDL_NAME = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "MDL_NAME = 'dslim/bert-base-NER'\n",
    "classifier = pipeline(task=\"ner\",\n",
    "                model=BASE_FOLDER + MDL_NAME)\n",
    "\n",
    "preds = classifier(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f2f6a4a-7b4b-470d-b096-82c1d5dda550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'positive',\n",
       "  'score': 0.37662995,\n",
       "  'word': 'Hu',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'neutral',\n",
       "  'score': 0.35554093,\n",
       "  'word': '##gging',\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'entity_group': 'positive',\n",
       "  'score': 0.38128707,\n",
       "  'word': 'Face is a French company based in New York City.',\n",
       "  'start': 8,\n",
       "  'end': 56}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"ner\", model=BASE_FOLDER + MDL_NAME, grouped_entities=True)\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6b619c-afba-40b2-8d52-410b62c9ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at /Users/mtide/Documents/code/AI/models/google-bert/bert-large-cased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9517, start: 30, end: 54, answer: huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "# 3. Question Answering\n",
    "MDL_NAME = 'google-bert/bert-large-cased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=BASE_FOLDER + MDL_NAME)\n",
    "\n",
    "preds = question_answerer(\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c833e7e-496e-4aef-909a-05c2f0060cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0d8b853f2f4c309aa1cc649c7913cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'We present the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-de'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Summarization\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"google/flan-t5-xl\",\n",
    "                      min_length=8,\n",
    "                      max_length=32,\n",
    ")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf0d041-69d6-45a2-a2eb-689171fcb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/mtide/Documents/code/AI/models/superb/hubert-base-superb-ks were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at /Users/mtide/Documents/code/AI/models/superb/hubert-base-superb-ks and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8031, 'label': '_unknown_'},\n",
       " {'score': 0.055, 'label': 'right'},\n",
       " {'score': 0.046, 'label': 'down'},\n",
       " {'score': 0.0349, 'label': 'go'},\n",
       " {'score': 0.0269, 'label': 'no'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Audio classification\n",
    "MDL_NAME = 'superb/hubert-base-superb-ks'\n",
    "\n",
    "classifier = pipeline(task=\"audio-classification\", model=BASE_FOLDER + MDL_NAME)\n",
    "\n",
    "preds = classifier(BASE_FOLDER + '../data/mlk.flac')\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00463a6a-1aec-4166-920a-39285d19d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFResNetForImageClassification.\n",
      "\n",
      "All the layers of TFResNetForImageClassification were initialized from the model checkpoint at /Users/mtide/Documents/code/AI/models/microsoft/resnet-50.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFResNetForImageClassification for predictions without further training.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5874, 'label': 'lynx, catamount'}\n",
      "{'score': 0.1289, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.075, 'label': 'marmot'}\n",
      "{'score': 0.0382, 'label': 'badger'}\n",
      "{'score': 0.0131, 'label': 'Egyptian cat'}\n"
     ]
    }
   ],
   "source": [
    "# 7. Image Classificaiton\n",
    "MDL_NAME = 'microsoft/resnet-50'\n",
    "\n",
    "classifier = pipeline(task=\"image-classification\", model=BASE_FOLDER + MDL_NAME)\n",
    "\n",
    "preds = classifier(\n",
    "    BASE_FOLDER + '../data/pipeline-cat-chonk.jpeg'\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d98e31-49ba-499e-9ba7-624844e4fdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
